{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoL0x0y7C516"
      },
      "source": [
        "# AutoGenKernel: Generating Linux Kernel Code with Character-Level RNNs\n",
        "\n",
        "## Objective\n",
        "To train a Char RNN model to generate syntactically plausible and thematically relevant Linux kernel code snippets, aiming to explore the model's ability to capture the intricacies of system-level programming languages and potentially aid in the automation of low-level code writing.\n",
        "\n",
        "## Background and Motivation\n",
        "The Linux kernel, with its extensive and complex code base, represents a challenging dataset for machine learning models. By attempting to generate kernel code, this project aims to push the boundaries of what's possible with Char RNNs in understanding and replicating the structure and syntax of programming languages, particularly C.\n",
        "\n",
        "# Method\n",
        "1. Dataset Collection: Collect a substantial corpus of Linux kernel source code. This may involve downloading the latest stable Linux kernel source from the official repository and possibly additional modules and patches.\n",
        "2. Preprocessing: Clean the dataset for training. This includes removing comments, non-executable lines, and possibly segmenting the code into smaller, more manageable pieces.\n",
        "3. Model Architecture: Utilize a Char RNN model, potentially experimenting with different architectures such as LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Units) layers, to find the most effective configuration for this task.\n",
        "4. Training: Train the model on the processed Linux kernel source code, adjusting hyperparameters such as learning rate, batch size, and the number of layers to optimize performance.\n",
        "\n",
        "## Experiment\n",
        "- Hyperparameter Tuning: Systematically vary model hyperparameters to identify configurations that produce the most coherent and accurate code snippets.\n",
        "- Generation: Generate Linux kernel code snippets at various points during and after training to evaluate the model's progress and final performance. Use different \"temperatures\" in the softmax layer to control the randomness of the generated code.\n",
        "- Evaluation: Qualitatively assess the generated code's syntactical correctness, thematic relevance, and any emergent patterns or structures indicative of the model's understanding of kernel development.\n",
        "\n",
        "## Expected Challenges\n",
        "- Complexity of the Linux Kernel: The Linux kernel's code is highly complex, which may pose a significant challenge in generating coherent and plausible code snippets.\n",
        "- Evaluation Metrics: Quantitatively evaluating the quality of generated code is inherently challenging, requiring innovative approaches to assess both syntax and semantic relevance.\n",
        "\n",
        "## Conclusion\n",
        "The project will conclude with an analysis of the Char RNN's ability to generate Linux kernel code, discussing the quality of the generated code, the model's limitations, and potential applications of this technology in assisting kernel developers or educating new programmers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cVFVXRQMhBb"
      },
      "source": [
        "# 1. Environment Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XVOgL1qiKo6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0cb5583-8caf-493a-e675-28c2fca744da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries (Uncomment the line according to your preference)\n",
        "!pip install numpy tensorflow matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1W3MIRPM-Wb"
      },
      "source": [
        "# 2. Dataset Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHCMlSsdM9XR",
        "outputId": "de9dc6d3-c354-4e22-d0f2-6a5ed71be21b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded Linux Kernel source code version 5.10\n",
            "Extracted to linux_kernel_dataset/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Specify the version of the Linux Kernel you want to download\n",
        "kernel_version = '5.10'\n",
        "url = f'https://cdn.kernel.org/pub/linux/kernel/v5.x/linux-{kernel_version}.tar.xz'\n",
        "\n",
        "# Create a directory for the dataset if it doesn't already exist\n",
        "dataset_dir = 'linux_kernel_dataset'\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# Define the path for the downloaded file\n",
        "download_path = os.path.join(dataset_dir, f'linux-{kernel_version}.tar.xz')\n",
        "\n",
        "# Download the Linux Kernel source code\n",
        "response = requests.get(url)\n",
        "with open(download_path, 'wb') as file:\n",
        "    file.write(response.content)\n",
        "print(f'Downloaded Linux Kernel source code version {kernel_version}')\n",
        "\n",
        "# Extract the downloaded file\n",
        "import tarfile\n",
        "\n",
        "# Check if the downloaded file is a tar.xz file and extract it\n",
        "if download_path.endswith('.tar.xz'):\n",
        "    with tarfile.open(download_path, 'r:xz') as tar:\n",
        "        tar.extractall(dataset_dir)\n",
        "    print(f'Extracted to {dataset_dir}/')\n",
        "else:\n",
        "    print('Downloaded file is not a tar.xz file. Please check the file format.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTI5NvWhPXoX"
      },
      "source": [
        "# 3. Preprocessing the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aMDoEAkbMvwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46203027-b19f-41c7-f527-e26a7b050571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All code has been processed and saved to processed_linux_kernel_code.txt\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "\n",
        "def remove_comments_and_empty_lines(code):\n",
        "    \"\"\"Remove comments and empty lines from the source code.\"\"\"\n",
        "    code = re.sub(re.compile(\"/\\*.*?\\*/\", re.DOTALL), \"\", code)  # remove block comments\n",
        "    code = re.sub(re.compile(\"//.*?\\n\" ), \"\", code)  # remove line comments\n",
        "    code = os.linesep.join([s for s in code.splitlines() if s.strip()])  # remove empty lines\n",
        "    return code\n",
        "\n",
        "def preprocess_directory(source_dir, output_path):\n",
        "    \"\"\"Walk through the source directory and preprocess all .c and .h files.\"\"\"\n",
        "    all_code = []\n",
        "    for subdir, dirs, files in os.walk(source_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.c') or file.endswith('.h'):\n",
        "                file_path = os.path.join(subdir, file)\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        code = f.read()\n",
        "                        processed_code = remove_comments_and_empty_lines(code)\n",
        "                        all_code.append(processed_code)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "    # Concatenate all processed code into a single string\n",
        "    all_code_str = '\\n'.join(all_code)\n",
        "\n",
        "    # Optionally, save the concatenated code to an output file\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(all_code_str)\n",
        "    print(f\"All code has been processed and saved to {output_path}\")\n",
        "\n",
        "# Set the path to the directory containing the extracted Linux kernel source code\n",
        "source_dir = 'linux_kernel_dataset/linux-5.10'\n",
        "# Specify the output file path\n",
        "output_path = 'processed_linux_kernel_code.txt'\n",
        "\n",
        "# Preprocess the entire directory\n",
        "preprocess_directory(source_dir, output_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDwPKnhJP-eM"
      },
      "source": [
        "# 4. Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bAdP0khbPxtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2dc1b93-b051-4660-bca1-4b3a77227c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           25600     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 100)           102500    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5375076 (20.50 MB)\n",
            "Trainable params: 5375076 (20.50 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "        LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = 100  # This should be the size of the vocabulary, i.e., the number of unique characters.\n",
        "embedding_dim = 256  # Dimensionality of the embedding layer.\n",
        "rnn_units = 1024  # Number of units in the LSTM layer.\n",
        "batch_size = 64  # Training batch size.\n",
        "\n",
        "# Build the model\n",
        "model = build_model(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=batch_size)\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGxiXR8WQW0b"
      },
      "source": [
        "# 5. Data Preparation for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8mRZCPMQLts",
        "outputId": "b7bab4f3-a4aa-470b-cf20-2a14f4eb6ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "147 unique characters\n",
            "Input: #include <linux/rcupdate.h>\n",
            "#include <linux/slab.h>\n",
            "#include <linux/err.h>\n",
            "#include <linux/assoc_arr\n",
            "Target: include <linux/rcupdate.h>\n",
            "#include <linux/slab.h>\n",
            "#include <linux/err.h>\n",
            "#include <linux/assoc_arra\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'processed_linux_kernel_code.txt' is the file with preprocessed code\n",
        "with open('processed_linux_kernel_code.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# Convert all text into integers\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Print the first example of the dataset\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print('Input:', ''.join(idx2char[input_example.numpy()]))\n",
        "    print('Target:', ''.join(idx2char[target_example.numpy()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9MSzHsMQfFF"
      },
      "source": [
        "# 6. Model Compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x0qZ35XQQY67"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fca1MnnhRpRn"
      },
      "source": [
        "## 6.1 Batch and Shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2nPyRv2FRrFj"
      },
      "outputs": [],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvHwgnohTGD8"
      },
      "source": [
        "# 7. Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sPf-LKeZRwNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3558503b-41bf-45b8-d544-ae6e848154af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           37632     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (64, None, 147)           150675    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5435283 (20.73 MB)\n",
            "Trainable params: 5435283 (20.73 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "112258/112258 [==============================] - 1258s 11ms/step - loss: 0.8284\n",
            "Epoch 2/10\n",
            "112258/112258 [==============================] - 1251s 11ms/step - loss: 0.8465\n",
            "Epoch 3/10\n",
            "112258/112258 [==============================] - 1253s 11ms/step - loss: 0.8672\n",
            "Epoch 4/10\n",
            "112258/112258 [==============================] - 1252s 11ms/step - loss: 0.8824\n",
            "Epoch 5/10\n",
            "112258/112258 [==============================] - 1252s 11ms/step - loss: 0.8952\n",
            "Epoch 6/10\n",
            "112258/112258 [==============================] - 1252s 11ms/step - loss: 0.9070\n",
            "Epoch 7/10\n",
            "112258/112258 [==============================] - 1251s 11ms/step - loss: 0.9160\n",
            "Epoch 8/10\n",
            "112258/112258 [==============================] - 1250s 11ms/step - loss: 0.9235\n",
            "Epoch 9/10\n",
            "112258/112258 [==============================] - 1251s 11ms/step - loss: 0.9309\n",
            "Epoch 10/10\n",
            "112258/112258 [==============================] - 1252s 11ms/step - loss: 0.9387\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "\n",
        "# Load and prepare the dataset\n",
        "with open('processed_linux_kernel_code.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Mapping characters to integers\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Preparing the input and target text\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Creating training batches\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Building the model\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "        LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(vocab_size=len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Model compilation\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Display the model's architecture\n",
        "model.summary()\n",
        "\n",
        "# Training the model\n",
        "EPOCHS = 10\n",
        "history = model.fit(dataset, epochs=EPOCHS)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model weights\n",
        "model.save_weights('./my_model_weights/my_model_weights')\n"
      ],
      "metadata": {
        "id": "O39UiLQPANXS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl0GHEEVXFwN"
      },
      "source": [
        "# 7. Model Evaluation and Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "c7dGQdIkXCZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a23695-ae80-410c-eda9-e1601c41b057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/* Linux Kernel */\n",
            "\t\t\t  ssp_port[1];\n",
            "\t\twhile (line6 && count_val)\n",
            "\t\t\t\t? change : format;\n",
            "\telse\n",
            "\t\tval = kzalloc(sizeof(chip));\n",
            "\tvp->pages[n].value[0] = (int)(mask & 0x0000FSI ? \"correct.\", count);\n",
            "\tcapture_temp(chip, CS8427_RESET, 0xf0000000);\n",
            "}\n",
            "static void snd_cs4233_sq_prepare(struct queue_irqsave(&chip->hw.resolution, interrupt))\n",
            "\t\treturn -ENXIO;\n",
            "\tvxp = kmalloc(x->ext, mpu_irqsave.integer, module_type, vendor_id & 0xff) | \\\n",
            "          (((xdp)->ack1->image[i + 0x80,\tretcontrol->next = (kcontrol->play + mix->io_switch.reble) << 4;\n",
            "\tucontrol->value.integer.value[0] =\n",
            "\t    ||\n",
            "\t    (!mbdmac_streams_writew(chip, MASK_ADDR_20;\n",
            "\t\t\tbreak;\n",
            "\t\t}\n",
            "\t}\n",
            "\tif (chip->name(chip) != 0) {\n",
            "\t\tpr_err(\"amd590a failed\\n\");\n",
            "\t\t\tpipeid = MAX_PAUSE_CTRL_MASK\t\t\t(7 << 0)\n",
            "#define PS3_AUDIO_INTERNAL\t\t(1<<0)\n",
            "#define PMAC_ERRSA_RECORD\t\t32\n",
            "#define AU8514_REV_ALL\t\t10\n",
            "#define BLKSELINPUT1B5_VOLUME_RESUME\t\t1\n",
            "#define GF1_RIRQ_45\t(1<<1) \n",
            "#define VIOD_MAP_TX_FORMAT\t\t\tsCapped ? DACRDEV :\n",
            "\t\t\t(gus->input_dev) / DBX_CTL;\n",
            "\t}\n",
            "#endif\n",
            "\treturn PTR_ERR(gus)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Assuming vocab, char2idx, idx2char are already defined from your dataset preparation steps\n",
        "\n",
        "# Function to build the model\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(model, start_string, generation_length=1000):\n",
        "    # Convert start_string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Here we reset the states of the model; this will clear the hidden states of the LSTM layer.\n",
        "    model.reset_states()\n",
        "\n",
        "    for _ in range(generation_length):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # Using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / 1.0  # The temperature parameter\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Parameters for rebuilding the model\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "# Rebuild the model with batch_size=1\n",
        "model = build_model(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=1)\n",
        "\n",
        "# Assuming you have saved your trained model weights\n",
        "model.load_weights('./my_model_weights/my_model_weights')\n",
        "\n",
        "# Build the model by specifying the input shape\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "# Generate text\n",
        "generated_text = generate_text(model, start_string=u\"/* Linux Kernel */\\n\", generation_length=1000)\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "vX7laY-cBdst"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZChWR_MXnbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2a1564-a000-4eb2-d09b-66ec2809f9b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 Complete [00h 30m 15s]\n",
            "loss: 1.0790519714355469\n",
            "\n",
            "Best loss So Far: 1.0790519714355469\n",
            "Total elapsed time: 00h 30m 15s\n",
            "\n",
            "Search: Running Trial #2\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "3                 |1                 |layers\n",
            "1024              |256               |lstm_units_0\n",
            "0.00012892        |0.0058172         |learning_rate\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "2                 |2                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Epoch 1/2\n",
            " 83718/112258 [=====================>........] - ETA: 9:16 - loss: 1.0096"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner\n",
        "import kerastuner as kt\n",
        "\n",
        "def model_builder(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Embedding(input_dim=len(vocab), output_dim=256, batch_input_shape=[None, None]))\n",
        "    for i in range(hp.Int('layers', 1, 3)):  # Number of layers\n",
        "        model.add(tf.keras.layers.LSTM(hp.Int(f'lstm_units_{i}', min_value=256, max_value=1024, step=256), return_sequences=True))\n",
        "    model.add(tf.keras.layers.Dense(len(vocab)))\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "    return model\n",
        "\n",
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='loss',\n",
        "                     max_epochs=10,\n",
        "                     directory='keras_tuner_dir',\n",
        "                     project_name='autogenkernel')\n",
        "\n",
        "# Start the tuning process\n",
        "tuner.search(dataset, epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a code snippet after hyperparameter tuning\n",
        "generated_text = generate_text(model, start_string=u\"/* Linux Kernel */\\n\", generation_length=1000)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "e7M2kgH0Bg5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J5nr8JLiB1J_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}